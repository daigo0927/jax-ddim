{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40e359ee-a91a-47b9-a98e-c075558786bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2021 NVIDIA Corporation\n",
      "Built on Mon_May__3_19:15:13_PDT_2021\n",
      "Cuda compilation tools, release 11.3, V11.3.109\n",
      "Build cuda_11.3.r11.3/compiler.29920130_0\n"
     ]
    }
   ],
   "source": [
    "! nvcc -V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0df68364-4c70-4701-bbd9-d0afa7d85b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n",
      "Collecting jax[cuda]\n",
      "  Using cached jax-0.3.14-py3-none-any.whl\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from jax[cuda]) (4.2.0)\n",
      "Requirement already satisfied: absl-py in /opt/conda/lib/python3.7/site-packages (from jax[cuda]) (1.0.0)\n",
      "Requirement already satisfied: numpy>=1.19 in /opt/conda/lib/python3.7/site-packages (from jax[cuda]) (1.21.6)\n",
      "Requirement already satisfied: scipy>=1.5 in /opt/conda/lib/python3.7/site-packages (from jax[cuda]) (1.7.3)\n",
      "Collecting etils[epath]\n",
      "  Using cached etils-0.6.0-py3-none-any.whl (98 kB)\n",
      "Requirement already satisfied: opt-einsum in /opt/conda/lib/python3.7/site-packages (from jax[cuda]) (3.3.0)\n",
      "Collecting jaxlib==0.3.14+cuda11.cudnn82\n",
      "  Using cached https://storage.googleapis.com/jax-releases/cuda11/jaxlib-0.3.14%2Bcuda11.cudnn82-cp37-none-manylinux2014_x86_64.whl (161.9 MB)\n",
      "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /opt/conda/lib/python3.7/site-packages (from jaxlib==0.3.14+cuda11.cudnn82->jax[cuda]) (1.12)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from absl-py->jax[cuda]) (1.16.0)\n",
      "Requirement already satisfied: importlib_resources in /opt/conda/lib/python3.7/site-packages (from etils[epath]->jax[cuda]) (5.7.1)\n",
      "Requirement already satisfied: zipp in /opt/conda/lib/python3.7/site-packages (from etils[epath]->jax[cuda]) (3.8.0)\n",
      "Installing collected packages: etils, jaxlib, jax\n",
      "Successfully installed etils-0.6.0 jax-0.3.14 jaxlib-0.3.14+cuda11.cudnn82\n",
      "Requirement already satisfied: flax in /home/jupyter/.local/lib/python3.7/site-packages (0.5.2)\n",
      "Requirement already satisfied: typing-extensions>=4.1.1 in /opt/conda/lib/python3.7/site-packages (from flax) (4.2.0)\n",
      "Requirement already satisfied: msgpack in /home/jupyter/.local/lib/python3.7/site-packages (from flax) (1.0.4)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.7/site-packages (from flax) (3.5.2)\n",
      "Requirement already satisfied: rich~=11.1 in /home/jupyter/.local/lib/python3.7/site-packages (from flax) (11.2.0)\n",
      "Requirement already satisfied: numpy>=1.12 in /opt/conda/lib/python3.7/site-packages (from flax) (1.21.6)\n",
      "Requirement already satisfied: PyYAML>=5.4.1 in /opt/conda/lib/python3.7/site-packages (from flax) (6.0)\n",
      "Requirement already satisfied: optax in /home/jupyter/.local/lib/python3.7/site-packages (from flax) (0.1.3)\n",
      "Requirement already satisfied: jax>=0.3.2 in /opt/conda/lib/python3.7/site-packages (from flax) (0.3.14)\n",
      "Requirement already satisfied: etils[epath] in /opt/conda/lib/python3.7/site-packages (from jax>=0.3.2->flax) (0.6.0)\n",
      "Requirement already satisfied: scipy>=1.5 in /opt/conda/lib/python3.7/site-packages (from jax>=0.3.2->flax) (1.7.3)\n",
      "Requirement already satisfied: opt-einsum in /opt/conda/lib/python3.7/site-packages (from jax>=0.3.2->flax) (3.3.0)\n",
      "Requirement already satisfied: absl-py in /opt/conda/lib/python3.7/site-packages (from jax>=0.3.2->flax) (1.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in /opt/conda/lib/python3.7/site-packages (from rich~=11.1->flax) (2.12.0)\n",
      "Requirement already satisfied: commonmark<0.10.0,>=0.9.0 in /home/jupyter/.local/lib/python3.7/site-packages (from rich~=11.1->flax) (0.9.1)\n",
      "Requirement already satisfied: colorama<0.5.0,>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from rich~=11.1->flax) (0.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib->flax) (21.3)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib->flax) (4.33.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib->flax) (0.11.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib->flax) (9.1.1)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->flax) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.7/site-packages (from matplotlib->flax) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->flax) (1.4.2)\n",
      "Requirement already satisfied: chex>=0.0.4 in /home/jupyter/.local/lib/python3.7/site-packages (from optax->flax) (0.1.3)\n",
      "Requirement already satisfied: jaxlib>=0.1.37 in /opt/conda/lib/python3.7/site-packages (from optax->flax) (0.3.14+cuda11.cudnn82)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from absl-py->jax>=0.3.2->flax) (1.16.0)\n",
      "Requirement already satisfied: dm-tree>=0.1.5 in /opt/conda/lib/python3.7/site-packages (from chex>=0.0.4->optax->flax) (0.1.7)\n",
      "Requirement already satisfied: toolz>=0.9.0 in /home/jupyter/.local/lib/python3.7/site-packages (from chex>=0.0.4->optax->flax) (0.12.0)\n",
      "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /opt/conda/lib/python3.7/site-packages (from jaxlib>=0.1.37->optax->flax) (1.12)\n",
      "Requirement already satisfied: importlib_resources in /opt/conda/lib/python3.7/site-packages (from etils[epath]->jax>=0.3.2->flax) (5.7.1)\n",
      "Requirement already satisfied: zipp in /opt/conda/lib/python3.7/site-packages (from etils[epath]->jax>=0.3.2->flax) (3.8.0)\n",
      "flax                                  0.5.2\n",
      "jax                                   0.3.14\n",
      "jaxlib                                0.3.14+cuda11.cudnn82\n",
      "jupyter-server-mathjax                0.2.5\n",
      "optax                                 0.1.3\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import jax\n",
    "except ModuleNotFoundError:\n",
    "    ! pip install --user --upgrade \"jax[cuda]\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n",
    "    ! pip install --user flax\n",
    "    \n",
    "! pip list | grep ax    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e79ea0b-7852-42b9-bf9b-c219fadd0fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from typing import Tuple, Any\n",
    "from datetime import datetime\n",
    "from functools import partial\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax import linen as nn\n",
    "from flax.training import train_state\n",
    "from flax.training.checkpoints import save_checkpoint\n",
    "import optax\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "from model import DiffusionModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "970ae14e-58a4-483e-b9b7-7bafacfc1b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_output_dir(output_dir: Path) -> Tuple[Path, Path, Path]:\n",
    "    output_dir = output_dir / datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "    ckpt_dir = output_dir / 'models'\n",
    "    log_dir = output_dir / 'logs'\n",
    "    \n",
    "    if not output_dir.exists():\n",
    "        output_dir.mkdir(parents=True)\n",
    "        ckpt_dir.mkdir()\n",
    "        log_dir.mkdir()\n",
    "\n",
    "    return (output_dir, ckpt_dir, log_dir)\n",
    "\n",
    "\n",
    "def preprocess_image(data, image_size):\n",
    "    image = data['image']\n",
    "    height = tf.shape(image)[0]\n",
    "    width = tf.shape(image)[1]\n",
    "    crop_size = tf.minimum(height, width)\n",
    "    image = tf.image.crop_to_bounding_box(image,\n",
    "                                          (height - crop_size) // 2,\n",
    "                                          (width - crop_size) // 2,\n",
    "                                          crop_size,\n",
    "                                          crop_size)\n",
    "    # resize and clip\n",
    "    # for image downsampling it is important to turn on antialiasing\n",
    "    image = tf.image.resize(image, size=(image_size, image_size),\n",
    "                            antialias=True)\n",
    "    return tf.clip_by_value(image / 255.0, 0.0, 1.0)\n",
    "\n",
    "\n",
    "def prepare_datasets(image_size: int = 64,\n",
    "                     batch_size: int = 64):\n",
    "    dataset_name = 'oxford_flowers102'\n",
    "    split_train = 'train[:80%]+validation[:80%]+test[:80%]'\n",
    "    split_val = 'train[80%:]+validation[80%:]+test[80%:]'\n",
    "\n",
    "    preprocess_fn = partial(preprocess_image, image_size=image_size)\n",
    "    \n",
    "    ds_train = tfds.load(dataset_name, split=split_train, shuffle_files=True)\\\n",
    "                   .map(preprocess_fn, num_parallel_calls=tf.data.AUTOTUNE)\\\n",
    "                   .cache()\\\n",
    "                   .shuffle(buffer_size=10*batch_size)\\\n",
    "                   .batch(batch_size, drop_remainder=True)\\\n",
    "                   .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    ds_train = tfds.as_numpy(ds_train)\n",
    "                   \n",
    "    ds_val = tfds.load(dataset_name, split=split_val, shuffle_files=True)\\\n",
    "                 .map(preprocess_fn, num_parallel_calls=tf.data.AUTOTUNE)\\\n",
    "                 .cache()\\\n",
    "                 .batch(batch_size, drop_remainder=True)\\\n",
    "                 .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    ds_val = tfds.as_numpy(ds_val)\n",
    "\n",
    "    return ds_train, ds_val\n",
    "\n",
    "\n",
    "class TrainState(train_state.TrainState):\n",
    "    batch_stats: Any\n",
    "\n",
    "\n",
    "def model(**kwargs):\n",
    "    return DiffusionModel(**kwargs)\n",
    "\n",
    "\n",
    "def l1_loss(predictions, targets):\n",
    "    return jnp.abs(predictions - targets)\n",
    "\n",
    "\n",
    "def kernel_inception_distance():\n",
    "    raise NotImplementedError()\n",
    "\n",
    "\n",
    "def update_ema(p_cur, p_new, momentum: float = 0.999):\n",
    "    return momentum*p_cur + (1-momentum)*p_new\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def train_step(state, batch, rng):\n",
    "    def loss_fn(params):\n",
    "        outputs, mutated_vars = state.apply_fn(\n",
    "            {'params': params, 'batch_stats': state.batch_stats},\n",
    "            batch, rng, train=True,\n",
    "            mutable=['batch_stats']\n",
    "        )\n",
    "        noises, images, pred_noises, pred_images = outputs\n",
    "        \n",
    "        noise_loss = l1_loss(pred_noises, noises).mean()\n",
    "        image_loss = l1_loss(pred_images, images).mean()\n",
    "        loss = noise_loss + image_loss\n",
    "        return loss, mutated_vars\n",
    "    \n",
    "    grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
    "    (loss, mutated_vars), grads = grad_fn(state.params)\n",
    "    state = state.apply_gradients(\n",
    "        grads=grads,\n",
    "        batch_stats=mutated_vars['batch_stats'])\n",
    "    return state, loss\n",
    "        \n",
    "\n",
    "@partial(jax.jit, static_argnums=4)\n",
    "def evaluate(params,\n",
    "             batch_stats,\n",
    "             rng,\n",
    "             images,\n",
    "             diffusion_steps: int):\n",
    "    def eval_fn(model):\n",
    "        # TODO: quantitative metrics\n",
    "\n",
    "        generated_images = model.generate(rng, images.shape, diffusion_steps)\n",
    "        return generated_images\n",
    "\n",
    "    variables = {'params': params, 'batch_stats': batch_stats}\n",
    "    return nn.apply(eval_fn, model())(variables)\n",
    "\n",
    "\n",
    "def run(epochs: int,\n",
    "        image_size: int,\n",
    "        batch_size: int,\n",
    "        learning_rate: float,\n",
    "        weight_decay: float,\n",
    "        val_diffusion_steps: int,\n",
    "        output_dir: Path):\n",
    "    output_dir, ckpt_dir, log_dir = create_output_dir(output_dir)\n",
    "    summary_writer = tf.summary.create_file_writer(str(log_dir))\n",
    "    \n",
    "    rng = jax.random.PRNGKey(0)\n",
    "    rng, key_init, key_diffusion = jax.random.split(rng, 3)\n",
    "\n",
    "    ds_train, _ = prepare_datasets(image_size, batch_size)\n",
    "\n",
    "    image_shape = (batch_size, image_size, image_size, 3)\n",
    "    dummy = jnp.ones(image_shape, dtype=jnp.float32)\n",
    "\n",
    "    variables = model().init(key_init, dummy, key_diffusion,\n",
    "                             train=True)\n",
    "\n",
    "    state = TrainState.create(\n",
    "        apply_fn=model().apply,\n",
    "        params=variables['params'],\n",
    "        batch_stats=variables['batch_stats'],\n",
    "        tx=optax.adamw(learning_rate, weight_decay=weight_decay)\n",
    "    )\n",
    "    ema_params = state.params.copy(add_or_replace={})\n",
    "    rng, rng_train, rng_val = jax.random.split(rng, 3)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        losses = []\n",
    "        pbar = tqdm(ds_train, desc=f'Epoch {epoch}')\n",
    "        for images in pbar:\n",
    "            rng_train, key = jax.random.split(rng_train)\n",
    "            state, loss = train_step(state, images, key)\n",
    "\n",
    "            pbar.set_postfix({'loss': f'{loss:.5f}'})\n",
    "            losses.append(loss)\n",
    "            ema_params = jax.tree_map(update_ema, ema_params, state.params)\n",
    "\n",
    "        generated_images = evaluate(ema_params,\n",
    "                                    state.batch_stats,\n",
    "                                    rng=rng_val,\n",
    "                                    images=dummy,\n",
    "                                    diffusion_steps=val_diffusion_steps)\n",
    "\n",
    "        with summary_writer.as_default():\n",
    "            tf.summary.scalar('loss', np.mean(losses), step=epoch)\n",
    "            tf.summary.image('generated', generated_images, step=epoch,\n",
    "                             max_outputs=8)\n",
    "        save_checkpoint(ckpt_dir, state, step=epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c06908ac-1b2c-46fb-b7f7-65681eada0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'epochs': 50,\n",
    "    'image_size': 64,\n",
    "    'batch_size': 64,\n",
    "    'learning_rate': 1e-3,\n",
    "    'weight_decay': 1e-4,\n",
    "    'val_diffusion_steps': 20,\n",
    "    'output_dir': Path('./outputs')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7734cc6a-7046-4b37-ad73-0e1bbe8bf3da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 102/102 [00:22<00:00,  4.55it/s, loss=0.51078]\n",
      "Epoch 1: 100%|██████████| 102/102 [00:08<00:00, 11.43it/s, loss=0.42015]\n",
      "Epoch 2: 100%|██████████| 102/102 [00:08<00:00, 11.36it/s, loss=0.47383]\n",
      "Epoch 3: 100%|██████████| 102/102 [00:09<00:00, 11.30it/s, loss=0.36899]\n",
      "Epoch 4: 100%|██████████| 102/102 [00:08<00:00, 11.43it/s, loss=0.34633]\n",
      "Epoch 5: 100%|██████████| 102/102 [00:08<00:00, 11.53it/s, loss=0.36658]\n",
      "Epoch 6: 100%|██████████| 102/102 [00:08<00:00, 11.40it/s, loss=0.32866]\n",
      "Epoch 7: 100%|██████████| 102/102 [00:08<00:00, 11.43it/s, loss=0.30865]\n",
      "Epoch 8: 100%|██████████| 102/102 [00:08<00:00, 11.39it/s, loss=0.31624]\n",
      "Epoch 9: 100%|██████████| 102/102 [00:08<00:00, 11.35it/s, loss=0.31032]\n",
      "Epoch 10: 100%|██████████| 102/102 [00:08<00:00, 11.38it/s, loss=0.37706]\n",
      "Epoch 11: 100%|██████████| 102/102 [00:08<00:00, 11.45it/s, loss=0.32258]\n",
      "Epoch 12: 100%|██████████| 102/102 [00:08<00:00, 11.45it/s, loss=0.30739]\n",
      "Epoch 13: 100%|██████████| 102/102 [00:08<00:00, 11.44it/s, loss=0.32137]\n",
      "Epoch 14: 100%|██████████| 102/102 [00:08<00:00, 11.50it/s, loss=0.29868]\n",
      "Epoch 15: 100%|██████████| 102/102 [00:08<00:00, 11.52it/s, loss=0.28288]\n",
      "Epoch 16: 100%|██████████| 102/102 [00:08<00:00, 11.44it/s, loss=0.29708]\n",
      "Epoch 17: 100%|██████████| 102/102 [00:09<00:00, 11.32it/s, loss=0.31299]\n",
      "Epoch 18: 100%|██████████| 102/102 [00:08<00:00, 11.47it/s, loss=0.32118]\n",
      "Epoch 19: 100%|██████████| 102/102 [00:08<00:00, 11.40it/s, loss=0.29972]\n",
      "Epoch 20: 100%|██████████| 102/102 [00:08<00:00, 11.36it/s, loss=0.28772]\n",
      "Epoch 21: 100%|██████████| 102/102 [00:08<00:00, 11.37it/s, loss=0.30234]\n",
      "Epoch 22: 100%|██████████| 102/102 [00:08<00:00, 11.50it/s, loss=0.28559]\n",
      "Epoch 23: 100%|██████████| 102/102 [00:09<00:00, 11.24it/s, loss=0.29770]\n",
      "Epoch 24: 100%|██████████| 102/102 [00:08<00:00, 11.44it/s, loss=0.29021]\n",
      "Epoch 25: 100%|██████████| 102/102 [00:08<00:00, 11.39it/s, loss=0.31474]\n",
      "Epoch 26: 100%|██████████| 102/102 [00:08<00:00, 11.41it/s, loss=0.28491]\n",
      "Epoch 27: 100%|██████████| 102/102 [00:08<00:00, 11.41it/s, loss=0.28577]\n",
      "Epoch 28: 100%|██████████| 102/102 [00:09<00:00, 11.33it/s, loss=0.28820]\n",
      "Epoch 29: 100%|██████████| 102/102 [00:08<00:00, 11.47it/s, loss=0.28102]\n",
      "Epoch 30: 100%|██████████| 102/102 [00:08<00:00, 11.41it/s, loss=0.27701]\n",
      "Epoch 31: 100%|██████████| 102/102 [00:08<00:00, 11.49it/s, loss=0.28027]\n",
      "Epoch 32: 100%|██████████| 102/102 [00:08<00:00, 11.53it/s, loss=0.28872]\n",
      "Epoch 33: 100%|██████████| 102/102 [00:08<00:00, 11.48it/s, loss=0.30396]\n",
      "Epoch 34: 100%|██████████| 102/102 [00:08<00:00, 11.45it/s, loss=0.30711]\n",
      "Epoch 35: 100%|██████████| 102/102 [00:08<00:00, 11.48it/s, loss=0.30062]\n",
      "Epoch 36: 100%|██████████| 102/102 [00:08<00:00, 11.62it/s, loss=0.28206]\n",
      "Epoch 37: 100%|██████████| 102/102 [00:08<00:00, 11.47it/s, loss=0.28642]\n",
      "Epoch 38: 100%|██████████| 102/102 [00:08<00:00, 11.40it/s, loss=0.28119]\n",
      "Epoch 39: 100%|██████████| 102/102 [00:08<00:00, 11.46it/s, loss=0.28597]\n",
      "Epoch 40: 100%|██████████| 102/102 [00:08<00:00, 11.51it/s, loss=0.27644]\n",
      "Epoch 41: 100%|██████████| 102/102 [00:08<00:00, 11.54it/s, loss=0.27812]\n",
      "Epoch 42: 100%|██████████| 102/102 [00:08<00:00, 11.56it/s, loss=0.30760]\n",
      "Epoch 43: 100%|██████████| 102/102 [00:08<00:00, 11.51it/s, loss=0.33875]\n",
      "Epoch 44: 100%|██████████| 102/102 [00:08<00:00, 11.55it/s, loss=0.27597]\n",
      "Epoch 45: 100%|██████████| 102/102 [00:08<00:00, 11.38it/s, loss=0.28551]\n",
      "Epoch 46: 100%|██████████| 102/102 [00:08<00:00, 11.64it/s, loss=0.28587]\n",
      "Epoch 47: 100%|██████████| 102/102 [00:08<00:00, 11.59it/s, loss=0.28336]\n",
      "Epoch 48: 100%|██████████| 102/102 [00:08<00:00, 11.58it/s, loss=0.29836]\n",
      "Epoch 49: 100%|██████████| 102/102 [00:08<00:00, 11.62it/s, loss=0.29455]\n"
     ]
    }
   ],
   "source": [
    "tf.config.experimental.set_visible_devices([], 'GPU')\n",
    "\n",
    "run(**args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow 2 (Local)",
   "language": "python",
   "name": "local-tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
